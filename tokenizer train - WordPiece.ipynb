{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:31:14.620825Z",
     "start_time": "2022-03-30T01:31:14.317555Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pprint import pprint\n",
    "\n",
    "from soynlp.normalizer import repeat_normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:31:15.217775Z",
     "start_time": "2022-03-30T01:31:15.213682Z"
    },
    "id": "ahc_PV2F4WxX"
   },
   "outputs": [],
   "source": [
    "def dump_jsonl(data, output_path, append=False):\n",
    "    mode = \"a+\" if append else \"w\"\n",
    "    with open(output_path, mode, encoding=\"utf-8\") as f:\n",
    "        for line in data:\n",
    "            json_record = json.dumps(line, ensure_ascii=False)\n",
    "            f.write(json_record + \"\\n\")\n",
    "\n",
    "\n",
    "def load_jsonl(input_path) -> list:\n",
    "    data = []\n",
    "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            data.append(json.loads(line.rstrip(\"\\n|\\r\")))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:31:15.836942Z",
     "start_time": "2022-03-30T01:31:15.834513Z"
    }
   },
   "outputs": [],
   "source": [
    "os.chdir(\"/home/nsw0311/nas_storage\")\n",
    "\n",
    "vocab_size = 24000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoCW3M3_HeeV"
   },
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:30:18.874123Z",
     "start_time": "2022-02-28T01:30:18.868413Z"
    }
   },
   "outputs": [],
   "source": [
    "def processing(text):\n",
    "    text = re.sub(r\" +\", r\" \", text.strip())\n",
    "    text = re.sub(r\"(.{8,}?)\\1+\", r\"\\1\", text)\n",
    "    text = re.sub(\n",
    "        r\"[^ ㄱ-ㅎㅏ-ㅣ가-힣A-Za-z0-9~!@#$%\\^\\&*\\(\\)_\\+\\-=\\[\\]{},\\./<>\\?]\", r\"\", text\n",
    "    )\n",
    "    text = re.sub(r\"http.+\", r\"[URL]\", text)\n",
    "\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:05.599350Z",
     "start_time": "2022-02-28T01:30:20.504397Z"
    }
   },
   "outputs": [],
   "source": [
    "data = load_jsonl(\"datasets/RG/dialog_data.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:05.610968Z",
     "start_time": "2022-02-28T01:31:05.601177Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12753322, {'content': '<user1> 웅.. 서럽', 'talk_id': 'kakaotalk_000000'})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data), data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:05.624785Z",
     "start_time": "2022-02-28T01:31:05.612647Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'웅.. 서럽'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][\"content\"][8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:05.631566Z",
     "start_time": "2022-02-28T01:31:05.627346Z"
    }
   },
   "outputs": [],
   "source": [
    "def gen():\n",
    "    for row in data:\n",
    "        yield processing(row[\"content\"][8:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:16.185253Z",
     "start_time": "2022-02-28T01:31:16.164451Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]',\n",
      " '[UNK]',\n",
      " '[CLS]',\n",
      " '[SEP]',\n",
      " '[MASK]',\n",
      " '[BOS]',\n",
      " '[EOS]',\n",
      " '[TSEP]',\n",
      " '[NAME]',\n",
      " '[URL]',\n",
      " '[UNK0]',\n",
      " '[UNK1]',\n",
      " '[UNK2]',\n",
      " '[UNK3]',\n",
      " '[UNK4]',\n",
      " '[UNK5]',\n",
      " '[UNK6]',\n",
      " '[UNK7]',\n",
      " '[UNK8]',\n",
      " '[UNK9]',\n",
      " '[UNUSED0]',\n",
      " '[UNUSED1]',\n",
      " '[UNUSED2]',\n",
      " '[UNUSED3]',\n",
      " '[UNUSED4]',\n",
      " '[UNUSED5]',\n",
      " '[UNUSED6]',\n",
      " '[UNUSED7]',\n",
      " '[UNUSED8]',\n",
      " '[UNUSED9]',\n",
      " '[UNUSED10]',\n",
      " '[UNUSED11]',\n",
      " '[UNUSED12]',\n",
      " '[UNUSED13]',\n",
      " '[UNUSED14]',\n",
      " '[UNUSED15]',\n",
      " '[UNUSED16]',\n",
      " '[UNUSED17]',\n",
      " '[UNUSED18]',\n",
      " '[UNUSED19]',\n",
      " '[UNUSED20]',\n",
      " '[UNUSED21]',\n",
      " '[UNUSED22]',\n",
      " '[UNUSED23]',\n",
      " '[UNUSED24]',\n",
      " '[UNUSED25]',\n",
      " '[UNUSED26]',\n",
      " '[UNUSED27]',\n",
      " '[UNUSED28]',\n",
      " '[UNUSED29]',\n",
      " '[UNUSED30]',\n",
      " '[UNUSED31]',\n",
      " '[UNUSED32]',\n",
      " '[UNUSED33]',\n",
      " '[UNUSED34]',\n",
      " '[UNUSED35]',\n",
      " '[UNUSED36]',\n",
      " '[UNUSED37]',\n",
      " '[UNUSED38]',\n",
      " '[UNUSED39]',\n",
      " '[UNUSED40]',\n",
      " '[UNUSED41]',\n",
      " '[UNUSED42]',\n",
      " '[UNUSED43]',\n",
      " '[UNUSED44]',\n",
      " '[UNUSED45]',\n",
      " '[UNUSED46]',\n",
      " '[UNUSED47]',\n",
      " '[UNUSED48]',\n",
      " '[UNUSED49]',\n",
      " '[UNUSED50]',\n",
      " '[UNUSED51]',\n",
      " '[UNUSED52]',\n",
      " '[UNUSED53]',\n",
      " '[UNUSED54]',\n",
      " '[UNUSED55]',\n",
      " '[UNUSED56]',\n",
      " '[UNUSED57]',\n",
      " '[UNUSED58]',\n",
      " '[UNUSED59]',\n",
      " '[UNUSED60]',\n",
      " '[UNUSED61]',\n",
      " '[UNUSED62]',\n",
      " '[UNUSED63]',\n",
      " '[UNUSED64]',\n",
      " '[UNUSED65]',\n",
      " '[UNUSED66]',\n",
      " '[UNUSED67]',\n",
      " '[UNUSED68]',\n",
      " '[UNUSED69]',\n",
      " '[UNUSED70]',\n",
      " '[UNUSED71]',\n",
      " '[UNUSED72]',\n",
      " '[UNUSED73]',\n",
      " '[UNUSED74]',\n",
      " '[UNUSED75]',\n",
      " '[UNUSED76]',\n",
      " '[UNUSED77]',\n",
      " '[UNUSED78]',\n",
      " '[UNUSED79]',\n",
      " '[UNUSED80]',\n",
      " '[UNUSED81]',\n",
      " '[UNUSED82]',\n",
      " '[UNUSED83]',\n",
      " '[UNUSED84]',\n",
      " '[UNUSED85]',\n",
      " '[UNUSED86]',\n",
      " '[UNUSED87]',\n",
      " '[UNUSED88]',\n",
      " '[UNUSED89]',\n",
      " '[UNUSED90]',\n",
      " '[UNUSED91]',\n",
      " '[UNUSED92]',\n",
      " '[UNUSED93]',\n",
      " '[UNUSED94]',\n",
      " '[UNUSED95]',\n",
      " '[UNUSED96]',\n",
      " '[UNUSED97]',\n",
      " '[UNUSED98]',\n",
      " '[UNUSED99]']\n"
     ]
    }
   ],
   "source": [
    "user_defined_symbols = [\n",
    "    \"[PAD]\",\n",
    "    \"[UNK]\",\n",
    "    \"[CLS]\",\n",
    "    \"[SEP]\",\n",
    "    \"[MASK]\",\n",
    "    \"[BOS]\",\n",
    "    \"[EOS]\",\n",
    "    \"[TSEP]\",\n",
    "    \"[NAME]\",\n",
    "    \"[URL]\",\n",
    "]\n",
    "user_defined_symbols += [f\"[UNK{i}]\" for i in range(10)]\n",
    "unused_token_num = 100\n",
    "unused_list = [f\"[UNUSED{i}]\" for i in range(100)]\n",
    "whole_user_defined_symbols = user_defined_symbols + unused_list\n",
    "\n",
    "pprint(whole_user_defined_symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M78kc0QyHjW6"
   },
   "source": [
    "# train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:27.226189Z",
     "start_time": "2022-02-28T01:31:27.213352Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordPiece\n",
    "\n",
    "bert_tokenizer = Tokenizer(WordPiece(unk_token=\"[UNK]\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:38.583566Z",
     "start_time": "2022-02-28T01:31:38.578697Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers import normalizers\n",
    "from tokenizers.normalizers import Lowercase, NFD, StripAccents\n",
    "\n",
    "bert_tokenizer.normalizer = normalizers.Sequence([NFD(), Lowercase(), StripAccents()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:39.951116Z",
     "start_time": "2022-02-28T01:31:39.947288Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "bert_tokenizer.pre_tokenizer = Whitespace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:31:41.273368Z",
     "start_time": "2022-02-28T01:31:41.268391Z"
    }
   },
   "outputs": [],
   "source": [
    "from tokenizers.processors import TemplateProcessing\n",
    "\n",
    "bert_tokenizer.post_processor = TemplateProcessing(\n",
    "    single=\"[CLS] $A [SEP]\",\n",
    "    pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "    special_tokens=[(t, i) for i, t in enumerate(user_defined_symbols)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:44:00.048196Z",
     "start_time": "2022-02-28T01:31:42.250921Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tokenizers.trainers import WordPieceTrainer\n",
    "\n",
    "trainer = WordPieceTrainer(\n",
    "    vocab_size=vocab_size,\n",
    "    special_tokens=whole_user_defined_symbols,\n",
    ")\n",
    "bert_tokenizer.train_from_iterator(gen(), trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:44:00.056369Z",
     "start_time": "2022-02-28T01:44:00.050181Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 8541, 3472, 911, 614, 3972, 1459, 146, 3]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'테스트 ##용이 ##ᆫ데 잘 되는거 ##같아 ?'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = bert_tokenizer.encode(\"테스트용인데 잘 되는거같아?\")\n",
    "print(output.ids)\n",
    "\n",
    "bert_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:44:00.071199Z",
     "start_time": "2022-02-28T01:44:00.057864Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'테스트용인데 잘 되는거같아?'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tokenizers import decoders\n",
    "\n",
    "bert_tokenizer.decoder = decoders.WordPiece()\n",
    "bert_tokenizer.decode(output.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# convert transformers tokenizer and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:30:09.301876Z",
     "start_time": "2022-03-30T01:30:08.171252Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bert_tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_659516/3455814693.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mfast_tokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mElectraTokenizerFast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbert_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'bert_tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import ElectraTokenizerFast\n",
    "\n",
    "\n",
    "fast_tokenizer = ElectraTokenizerFast(tokenizer_object=bert_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:46:03.504487Z",
     "start_time": "2022-02-28T01:46:03.496017Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.pad_token = \"[PAD]\"\n",
    "fast_tokenizer.unk_token = \"[UNK]\"\n",
    "fast_tokenizer.cls_token = \"[CLS]\"\n",
    "fast_tokenizer.sep_token = \"[SEP]\"\n",
    "fast_tokenizer.mask_token = \"[MASK]\"\n",
    "fast_tokenizer.bos_token = \"[BOS]\"\n",
    "fast_tokenizer.eos_token = \"[EOS]\"\n",
    "\n",
    "special_tokens_dict = {\"additional_special_tokens\": user_defined_symbols}\n",
    "fast_tokenizer.add_special_tokens(special_tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:46:06.966327Z",
     "start_time": "2022-02-28T01:46:05.117266Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-28 10:46:05.244813: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[CLS] 테스트용 [SEP]'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.decode(fast_tokenizer.encode(\"테스트용\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-28T01:46:09.039000Z",
     "start_time": "2022-02-28T01:46:08.977106Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('etc/DialogWordPiece/tokenizer_config.json',\n",
       " 'etc/DialogWordPiece/special_tokens_map.json',\n",
       " 'etc/DialogWordPiece/vocab.txt',\n",
       " 'etc/DialogWordPiece/added_tokens.json',\n",
       " 'etc/DialogWordPiece/tokenizer.json')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fast_tokenizer.save_pretrained(\"etc/DialogWordPiece\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-03-30T01:32:00.325233Z",
     "start_time": "2022-03-30T01:32:00.292807Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nsw0311/.pyenv/versions/3.9.7/envs/main/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2267: UserWarning: `max_length` is ignored when `padding`=`True` and there is no truncation strategy. To pad to max length, use `padding='max_length'`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[2, 8541, 729, 3, 614, 4233, 3]], 'token_type_ids': [[0, 0, 0, 0, 1, 1, 1]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = ElectraTokenizerFast.from_pretrained(\"etc/DialogWordPiece\")\n",
    "\n",
    "t(\n",
    "    [[\"테스트용\", \"잘 되나\"]],\n",
    "    max_length=10,\n",
    "    padding=True,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HoCW3M3_HeeV",
    "M78kc0QyHjW6",
    "ETFAiEUiHgrG"
   ],
   "name": "evolved_transformer.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
